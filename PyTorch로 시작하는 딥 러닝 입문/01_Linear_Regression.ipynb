{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 선형 회귀(Linear Regression)\n",
        "\n",
        "선형으로 나타나는 데이터에서 보편적인 방정식을 찾아내 모델(함수)를 만드는 것. \n",
        "\n",
        "y = Wx + b에서 W와 b를 찾는것. (손실함수와 최적화를 통해)"
      ],
      "metadata": {
        "id": "3pDYWDcTz_Hf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dF3YtCDXytu9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laAWUcS1y3h3",
        "outputId": "ffd839e7-feaa-4bd1-c13c-41c79f535dfa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9c84f29950>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
        "y_train = torch.FloatTensor([[2], [4], [6]]) "
      ],
      "metadata": {
        "id": "Y8wmTpr0zDou"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 둘의 관계를 나타내는 함수를 정의하고 싶은 것이다. 즉, 최종 목적은 **일부의 데이터**로 일반화하여 어떤 x가 들어오더라도 y값을 예측할 수 있도록 하는 것이다."
      ],
      "metadata": {
        "id": "drKkTfba2qsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train)\n",
        "print(x_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT1CegJMzINw",
        "outputId": "bb80b479-0ed6-4e4a-c01d-d098948f9c4d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.]])\n",
            "torch.Size([3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB8Q1ilgzd7-",
        "outputId": "e7dc0e77-6809-4990-9127-ab0a02ea815e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.],\n",
            "        [4.],\n",
            "        [6.]])\n",
            "torch.Size([3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치를 0으로 초기화\n",
        "W = torch.zeros(1, requires_grad=True) # requires_grad -> 학습을 통해 값이 변경되는 텐서임을 알려줌."
      ],
      "metadata": {
        "id": "guUi1JgfzlW2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0Z2Fkhgz8MO",
        "outputId": "f4d791ca-57c9-455c-ef15-7651d696cab8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 편향을 0으로 초기화\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wa0LOCURz9ge",
        "outputId": "565653b9-0ff1-4e85-8e30-31cec81ecca9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재 직선의 방정식은 \n",
        "y = 0*x + 0 임"
      ],
      "metadata": {
        "id": "TKGJg19F1Nhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hypothesis : 가설 = 직선의 방정식\n",
        "hypothesis = x_train * W + b\n",
        "print(hypothesis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf01tKHV1W1P",
        "outputId": "9fd50d7a-de8d-41b2-fb6f-c7d815080be4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "가중치(기울기)와 편향(절편)이 모두 0이니  당연히 모든 y값도 0."
      ],
      "metadata": {
        "id": "S7T2UEub1nPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 비용 함수 \"평균 제곱 오차\" 선언\n",
        "cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "print(cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fhtL1Iu2Ome",
        "outputId": "d871933c-c6b6-4d3f-9b6f-0bfd121e072d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 확률적 경사 하강법 <- 옵티마이저\n",
        "optimizer = optim.SGD([W, b], lr=0.01)"
      ],
      "metadata": {
        "id": "HU2-H4mh2m44"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서 lr은 learning rate의 약자로 학습률을 의미한다. \n",
        "\n",
        "새로운 가중치는 기존의 가중치에서 (학습률*손실함수를 w에 대해서 미분한 값)을 뺀 값으로 갱신된다. 따라서 학습률이 높으면 높을수록 가중치가 크게크게 갱신된다."
      ],
      "metadata": {
        "id": "cLeu5QhW3ZZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련\n",
        "\n",
        "nb_epochs = 1000\n",
        "\n",
        "for epoch in range(1, nb_epochs+1):\n",
        "  # 예측\n",
        "  hypothesis = x_train * W + b\n",
        "\n",
        "  # 손실 계산\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "  # 가중치 갱신\n",
        "  optimizer.zero_grad() # 그레디언트 초기화\n",
        "  cost.backward() # 역전파 연산 수행 \n",
        "  optimizer.step()\n",
        "\n",
        "  # 100 에포크마다 로그 출력\n",
        "  if epoch % 100 == 0:\n",
        "    print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(epoch, nb_epochs, W.item(), b.item(), cost.item()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocrM6Xqd3Y4X",
        "outputId": "64f5f2f6-7593-4866-93fe-40ea2a860c1d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  100/1000 W: 1.842, b: 0.358 Cost: 0.018483\n",
            "Epoch  200/1000 W: 1.876, b: 0.281 Cost: 0.011421\n",
            "Epoch  300/1000 W: 1.903, b: 0.221 Cost: 0.007058\n",
            "Epoch  400/1000 W: 1.923, b: 0.174 Cost: 0.004361\n",
            "Epoch  500/1000 W: 1.940, b: 0.137 Cost: 0.002695\n",
            "Epoch  600/1000 W: 1.953, b: 0.107 Cost: 0.001665\n",
            "Epoch  700/1000 W: 1.963, b: 0.084 Cost: 0.001029\n",
            "Epoch  800/1000 W: 1.971, b: 0.066 Cost: 0.000636\n",
            "Epoch  900/1000 W: 1.977, b: 0.052 Cost: 0.000393\n",
            "Epoch 1000/1000 W: 1.982, b: 0.041 Cost: 0.000243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "신기하지 않은가? \n",
        "\n",
        "비용이 점점 감소한다는 뜻은 예측과 정답이 거의 비슷해지고 있다는 것을 의미한다. 즉, 모델은 정답을 더 잘 맞추기 위해서 가중치와 절편을 점점 수정해나간다.\n"
      ],
      "metadata": {
        "id": "CUKWZ8n25foo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## optimizer.zero_grad()가 필요한 이유\n",
        "\n",
        "매 에포크마다 optimizer.zero_grad()를(그레디언트 초기화) 호출하는 이유는 무엇일까?\n",
        "\n",
        "파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있기 때문이다."
      ],
      "metadata": {
        "id": "PyIzrILI7Dsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "w = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "nb_epochs = 20\n",
        "for epoch in range(1, nb_epochs + 1):\n",
        "  z = 2*w\n",
        "  z.backward()\n",
        "  print('수식을 w로 미분한 값 : {}'.format(w.grad))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Upyg-oEy7OLp",
        "outputId": "96f25776-a74a-4392-c8e5-54e9558514c5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "수식을 w로 미분한 값 : 2.0\n",
            "수식을 w로 미분한 값 : 4.0\n",
            "수식을 w로 미분한 값 : 6.0\n",
            "수식을 w로 미분한 값 : 8.0\n",
            "수식을 w로 미분한 값 : 10.0\n",
            "수식을 w로 미분한 값 : 12.0\n",
            "수식을 w로 미분한 값 : 14.0\n",
            "수식을 w로 미분한 값 : 16.0\n",
            "수식을 w로 미분한 값 : 18.0\n",
            "수식을 w로 미분한 값 : 20.0\n",
            "수식을 w로 미분한 값 : 22.0\n",
            "수식을 w로 미분한 값 : 24.0\n",
            "수식을 w로 미분한 값 : 26.0\n",
            "수식을 w로 미분한 값 : 28.0\n",
            "수식을 w로 미분한 값 : 30.0\n",
            "수식을 w로 미분한 값 : 32.0\n",
            "수식을 w로 미분한 값 : 34.0\n",
            "수식을 w로 미분한 값 : 36.0\n",
            "수식을 w로 미분한 값 : 38.0\n",
            "수식을 w로 미분한 값 : 40.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "z를 w로 미분하면 2가 나온다. 그런데 그 값이 계속해서 더해지고(누적되고) 있다. 이렇게 되면 에포크를 반복할 때마다 누적된 그레디언트 값이 역전파 되므로 모델이 수렴하지 않을 것이다. 따라서 매 에포크마다 zero_grad()로 초기화를 해줘야 한다."
      ],
      "metadata": {
        "id": "sL8jD1rR7-an"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 자동 미분(Autograd)\n",
        "\n",
        "앞서 텐서에다가 required_grad를 True로 지정해줬다. 이는 텐서에 대한 기울기(미분값)을 저장하겠다는 의미이다. \n",
        "\n",
        "w.grad 에 w에 대해 미분한 값이 저장된다."
      ],
      "metadata": {
        "id": "0W_qXXnq8u9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "w = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "y = w**2\n",
        "z = 2*y + 5\n",
        "\n",
        "z.backward() # 역전파 연산에서 미분이 되고 미분값이 저장된다.\n",
        "\n",
        "print(print('수식을 w로 미분한 값 : {}'.format(w.grad)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXbm44398y0n",
        "outputId": "85ed5add-8765-4e0d-c555-6310be9f8a43"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "수식을 w로 미분한 값 : 8.0\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "즉 함수 z에 대하여 w=2일 때의 접선의 기울기 8이 저장된다는 뜻이다."
      ],
      "metadata": {
        "id": "f7Uz_YXk-g8_"
      }
    }
  ]
}